{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from keras.layers import Conv2DTranspose, MaxPooling2D, concatenate, Dropout, Conv2D\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\AI\\\\ML\\\\Pro\\\\Data\\\\train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\AI\\\\ML\\\\Pro\\\\Data\\\\depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"images\"] = [np.array(load_img(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\AI\\\\ML\\\\Pro\\\\Data\\\\train\\\\images\\\\{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\AI\\\\ML\\\\Pro\\\\Data\\\\train\\\\masks\\\\{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the salt coverage and salt coverage classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(train_df.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df[\"images\"][0])\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "\n",
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)\n",
    "print(len(x_train[0]))\n",
    "print(ids_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_from_imagge(image):\n",
    "    #pass\n",
    "    res = []\n",
    "    for row in image:\n",
    "        res += list(row.T[0])\n",
    "    return res    \n",
    "        \n",
    "        \n",
    "def data_prepare_for_ploting(x_train):\n",
    "    res = []\n",
    "    for image in x_train:\n",
    "        res.append(row_from_imagge(image))\n",
    "    return res    \n",
    "\n",
    "\n",
    "data_for_ploting = pd.DataFrame(data_prepare_for_ploting(x_train))\n",
    "data_for_ploting[\"y\"] = cov_train\n",
    "\n",
    "\n",
    "print(pd.DataFrame(data_prepare_for_ploting(x_train)))\n",
    "\n",
    "print(x_train[0][0].T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_data_for_ploting = pd.DataFrame(TSNE(n_components=2).fit_transform(data_for_ploting.values),columns=['x1', 'x2'])\n",
    "tsne_data_for_ploting[\"y\"] = cov_train\n",
    "points1 = tsne_data_for_ploting.plot.scatter(x='x1', y='x2', c='y',colormap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_data_for_ploting = pd.DataFrame(PCA(n_components=2).fit_transform(data_for_ploting.values),columns=['x1', 'x2'])\n",
    "tsne_data_for_ploting[\"y\"] = cov_train\n",
    "points1 = tsne_data_for_ploting.plot.scatter(x='x1', y='x2', c='y',colormap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model import *\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 128, 128, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 128, 128, 1])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2DTranspose\n",
    "from model import *\n",
    "\n",
    "\n",
    "\n",
    "def neural_network_model(input_layer, start_neurons=16):\n",
    "    \n",
    "    # 128 -> 64\n",
    "    print(input_layer)\n",
    "    conv1, weights = new_conv_layer(input_layer, 1, 3, start_neurons * 1, use_pooling=False)\n",
    "    print(conv1)\n",
    "    conv1, weights = new_conv_layer(conv1, 16, 3, start_neurons * 1, use_pooling=False)\n",
    "    print(conv1)\n",
    "    pool1 = max_poling(conv1)\n",
    "    print(pool1)\n",
    "    pool1 = tf.nn.dropout(pool1, 0.25)\n",
    "    print(pool1)\n",
    "    print(\"__________________________________________\")\n",
    "    # 64 -> 32    \n",
    "    conv2, weights = new_conv_layer(pool1,16, 3, start_neurons * 2, use_pooling=False)\n",
    "    print(conv2)\n",
    "    conv2, weights = new_conv_layer(conv2, 32, 3, start_neurons * 2, use_pooling=False)\n",
    "    print(conv2)\n",
    "    pool2 = max_poling(conv2)\n",
    "    print(pool2)\n",
    "    pool2 = tf.nn.dropout(pool2, 0.5)\n",
    "    print(pool2)\n",
    "    print(\"__________________________________________\")\n",
    "    # 32 -> 16    \n",
    "    conv3, weights = new_conv_layer(pool2, 32, 3, start_neurons * 4, use_pooling=False)\n",
    "    print(conv3)\n",
    "    conv3, weights = new_conv_layer(conv3, 64, 3, start_neurons * 4, use_pooling=False)\n",
    "    print(conv3)\n",
    "    pool3 = max_poling(conv3)\n",
    "    print(pool3)\n",
    "    pool3 = tf.nn.dropout(pool3, 0.5)\n",
    "    print(pool3)\n",
    "    print(\"__________________________________________\")\n",
    "    # 16 -> 8\n",
    "    conv4, weights = new_conv_layer(pool3, 64, 3, start_neurons * 8, use_pooling=False)\n",
    "    print(conv4)\n",
    "    conv4, weights = new_conv_layer(conv4, 128, 3, start_neurons * 8, use_pooling=False)\n",
    "    print(conv4)\n",
    "    pool4 = max_poling(conv4)\n",
    "    print(pool4)\n",
    "    pool4 = tf.nn.dropout(pool4, 0.5)\n",
    "    print(pool4)\n",
    "    print(\"__________________________________________\")\n",
    "    # Middle\n",
    "    conv_middle, weights = new_conv_layer(pool4, 128, 3, start_neurons * 16, use_pooling=False)\n",
    "    print(\"conv_middle = \",conv_middle)\n",
    "    conv_middle, weights = new_conv_layer(conv_middle, 256, 3, start_neurons * 16, use_pooling=False)\n",
    "    print(\"conv_middle = \",conv_middle)\n",
    "    print(\"__________________________________________\")\n",
    "    deconv4 = deconv2d(conv_middle, 3, 16, 128, 256, tf.nn.relu, \"DeConv2d_4\")\n",
    "    print(deconv4)\n",
    "    uconv4 = tf.concat([deconv4, conv4],-1)\n",
    "    print(uconv4)\n",
    "    uconv4 = tf.nn.dropout(uconv4,0.5)\n",
    "    print(uconv4)\n",
    "    uconv4, weights = new_conv_layer(uconv4, 256, 3, start_neurons * 8,use_pooling=False)\n",
    "    print(uconv4)\n",
    "    uconv4, weights = new_conv_layer(uconv4, 128, 3, start_neurons * 8,use_pooling=False)\n",
    "    print(uconv4)\n",
    "    print(\"__________________________________________\")\n",
    "    deconv3 =  deconv2d(uconv4, 3, 32, 64, 128, tf.nn.relu, \"DeConv2d_3\")\n",
    "    print(deconv3) # (None, 32, 32, 64)\n",
    "    uconv3 = tf.concat([deconv3, conv3],-1)\n",
    "    print(uconv3)\n",
    "    uconv3 = tf.nn.dropout(uconv3,0.5)\n",
    "    print(uconv3)\n",
    "    uconv3, weights = new_conv_layer(uconv3, 128, 3, start_neurons * 4,use_pooling=False)\n",
    "    print(uconv3)\n",
    "    uconv3, weights = new_conv_layer(uconv3, 64, 3, start_neurons * 4,use_pooling=False)\n",
    "    print(uconv3)\n",
    "    print(\"__________________________________________\")\n",
    "    deconv2 =  deconv2d(uconv3, 3, 64, 32, 64, tf.nn.relu, \"DeConv2d_2\") # (None, 64, 64, 32)\n",
    "    print(deconv2)\n",
    "    uconv2 = tf.concat([deconv2, conv2],-1)\n",
    "    print(uconv2)\n",
    "    uconv2 = tf.nn.dropout(uconv2,0.5)\n",
    "    print(uconv2)\n",
    "    uconv2, weights = new_conv_layer(uconv2, 64, 3, start_neurons * 2,use_pooling=False)\n",
    "    print(uconv2)\n",
    "    uconv2, weights = new_conv_layer(uconv2, 32, 3, start_neurons * 2,use_pooling=False)\n",
    "    print(uconv2)\n",
    "    print(\"__________________________________________\")\n",
    "    deconv1 =  deconv2d(uconv2, 3, 128, 16, 32, tf.nn.relu, \"DeConv2d_1\") # (None, 128, 128, 16)\n",
    "    print(deconv1)\n",
    "    uconv1 = tf.concat([deconv1, conv1], -1)\n",
    "    print(uconv1)\n",
    "    uconv1 =tf.nn.dropout(uconv1,0.5)\n",
    "    print(uconv1)\n",
    "    uconv1, weights = new_conv_layer(uconv1, 32, 3, start_neurons,use_pooling=False)\n",
    "    print(uconv1)\n",
    "    uconv1, weights = new_conv_layer(uconv1, 16, 3, start_neurons,use_pooling=False)\n",
    "    print(uconv1)\n",
    "    output_layer, weights = new_conv_layer(uconv1, 16, 1, 1,use_pooling=False)\n",
    "    print(output_layer)\n",
    "    print(\"__________________________________________\")\n",
    "    return output_layer \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "logits = neural_network_model(X, start_neurons=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n",
    "learning_rate = 0.00001\n",
    "training_epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def get_next_batch(x,y,num_batches,indx):\n",
    "    return np.split(x, num_batches)[indx], np.split(y, num_batches)[indx] \n",
    "\n",
    "# Define IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    try:\n",
    "        prec = []\n",
    "        for t in np.arange(0.5, 1.0, 0.05):\n",
    "            y_pred_ = tf.to_int32(y_pred > t)\n",
    "            score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "            K.get_session().run(tf.local_variables_initializer())\n",
    "            with tf.control_dependencies([up_opt]):\n",
    "                score = tf.identity(score)\n",
    "            prec.append(score)\n",
    "    except Exception:\n",
    "        print(\"mean_iou Error!\")\n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    " \n",
    "\n",
    "loss = mean_iou(Y, logits)\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "#sess = tf.Session()\n",
    "#sess.run(init)\n",
    "\n",
    "def train_model(x_train, y_train, num_epoch, num_batches, learning_rate, cost_func):\n",
    "    prediction = neural_network_model(x_train, 16)\n",
    "    cost = cost_func(y_train, y_pred)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimaze(cost)\n",
    "    \n",
    "    with tf.Session as sess:\n",
    "        sess.run(tf.initialize_all_veriables())\n",
    "        \n",
    "        for epoch in range(num_epoch):\n",
    "            epoch_loss = 0\n",
    "            for bath_ind in range(num_batches):\n",
    "                x, y = get_next_batch(x_train, y_train, num_batches, indx)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x:x, y:y})\n",
    "                epoch_loss += c\n",
    "            print(\"Epoch: \", epoch, \" completed ut of \", num_epoch,\" loss == \", epoch_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
